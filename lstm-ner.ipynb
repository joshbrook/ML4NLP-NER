{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LSTM for NER\n","\n","Adapted from [this](https://github.com/cltl/ma-ml4nlp-labs/blob/main/code/assignment3/lstm-ner.ipynb) notebook"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:12:39.176440300Z","start_time":"2023-12-10T14:12:39.132674Z"},"execution":{"iopub.execute_input":"2023-12-10T14:35:15.218913Z","iopub.status.busy":"2023-12-10T14:35:15.218610Z","iopub.status.idle":"2023-12-10T14:35:27.267628Z","shell.execute_reply":"2023-12-10T14:35:27.266811Z","shell.execute_reply.started":"2023-12-10T14:35:15.218886Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","from keras import Sequential\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n","from keras.utils import plot_model\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","import gensim.downloader as api"]},{"cell_type":"markdown","metadata":{},"source":["# Set path to data and embeddings:"]},{"cell_type":"code","execution_count":9,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:15:36.421358600Z","start_time":"2023-12-10T14:15:36.409126300Z"},"execution":{"iopub.execute_input":"2023-12-10T14:36:46.445568Z","iopub.status.busy":"2023-12-10T14:36:46.445168Z","iopub.status.idle":"2023-12-10T14:36:46.450337Z","shell.execute_reply":"2023-12-10T14:36:46.449362Z","shell.execute_reply.started":"2023-12-10T14:36:46.445538Z"},"trusted":true},"outputs":[],"source":["# conll data\n","path_train ='/kaggle/input/ner-data/conll2003.train.conll'\n","path_eval = '/kaggle/input/ner-data/conll2003.test.conll'\n","\n","paths = [path_train, path_eval]\n","\n","# change to test if you are evaluating on test:\n","eval_split = 'test'\n","\n","# model output path\n","output_path = 'lstm-out.csv'"]},{"cell_type":"markdown","metadata":{},"source":["# Data preparation"]},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:15:39.360913800Z","start_time":"2023-12-10T14:15:38.834486700Z"},"execution":{"iopub.execute_input":"2023-12-10T14:36:53.345209Z","iopub.status.busy":"2023-12-10T14:36:53.344338Z","iopub.status.idle":"2023-12-10T14:36:54.204518Z","shell.execute_reply":"2023-12-10T14:36:54.203619Z","shell.execute_reply.started":"2023-12-10T14:36:53.345174Z"},"trusted":true},"outputs":[],"source":["# connll data\n","\n","def convert_data(paths):\n","    \n","    data = []\n","    sent_id = 1\n","    for path in paths:\n","        split = path.split('.')[-2]\n","        with open(path) as infile:\n","            lines = infile.read().split('\\n')\n","        for n, line in enumerate(lines):\n","            ll = line.split('\\t')\n","            if len(ll) > 2:\n","                d = dict()\n","                d['Sentence #'] = f'Sentence: {sent_id}'\n","                d['Word'] = ll[0]\n","                d['POS'] = ll[1]\n","                d['Tag'] = ll[-1]\n","                d['Split'] = split\n","                data.append(d)\n","\n","            else:\n","                sent_id += 1\n","    data = pd.DataFrame(data)\n","    return data\n","\n","data = convert_data(paths)"]},{"cell_type":"markdown","metadata":{},"source":["### Map tokens and labels to indices"]},{"cell_type":"code","execution_count":41,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:15:51.085074100Z","start_time":"2023-12-10T14:15:51.010005100Z"},"execution":{"iopub.execute_input":"2023-12-10T15:50:35.979097Z","iopub.status.busy":"2023-12-10T15:50:35.978213Z","iopub.status.idle":"2023-12-10T15:50:36.035550Z","shell.execute_reply":"2023-12-10T15:50:36.034718Z","shell.execute_reply.started":"2023-12-10T15:50:35.979054Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["27316\n","9\n"]}],"source":["# map tokens and labels to indices\n","\n","def get_dict_map(data, token_or_tag, embedding_model=None):\n","    tok2idx = {}\n","    idx2tok = {}\n","    \n","    if token_or_tag == 'token':\n","        vocab = list(set(data['Word'].to_list()))\n","    else:\n","        vocab = list(set(data['Tag'].to_list()))\n","    \n","    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n","    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}   \n","    \n","    return tok2idx, idx2tok\n","\n","\n","token2idx, idx2token = get_dict_map(data, 'token')\n","tag2idx, idx2tag = get_dict_map(data, 'tag')\n","n_vocab = len(token2idx)\n","n_tags = len(tag2idx)\n","print(n_vocab)\n","print(n_tags)"]},{"cell_type":"markdown","metadata":{},"source":["# Integrating embeddings\n","\n","Change the path of the embedding model below to load your own GoogleNews vectors. "]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:16:37.599930500Z","start_time":"2023-12-10T14:15:53.018954Z"},"trusted":true},"outputs":[],"source":["# Load embedding model\n","# Change path to your path\n","w2v_model = api.load('word2vec-google-news-300')\n","\n","# Create embedding matrix with zero vectors for oov words\n","emb_dim = 300\n","embedding_matrix = np.zeros((len(token2idx) + 1, emb_dim))\n","print(embedding_matrix.shape)\n","for word, i in token2idx.items():\n","    # You may have to change the following line to:\n","    # if word in w2v_model:\n","    if word in w2v_model.key_to_index:\n","        embedding_vector = w2v_model[word]\n","    else:\n","        embedding_vector = None\n","        # If you want to check OOV words:\n","        #print('couldnt find:', word, i)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","        \n","emb_dim = embedding_matrix.shape[1]"]},{"cell_type":"code","execution_count":21,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:16:47.803719100Z","start_time":"2023-12-10T14:16:47.672121700Z"},"execution":{"iopub.execute_input":"2023-12-10T14:41:12.171417Z","iopub.status.busy":"2023-12-10T14:41:12.171017Z","iopub.status.idle":"2023-12-10T14:41:12.271372Z","shell.execute_reply":"2023-12-10T14:41:12.270495Z","shell.execute_reply.started":"2023-12-10T14:41:12.171387Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>Split</th>\n","      <th>Word_idx</th>\n","      <th>Tag_idx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>EU</td>\n","      <td>NNP</td>\n","      <td>B-ORG</td>\n","      <td>train</td>\n","      <td>7450</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>rejects</td>\n","      <td>VBZ</td>\n","      <td>O</td>\n","      <td>train</td>\n","      <td>12056</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>German</td>\n","      <td>JJ</td>\n","      <td>B-MISC</td>\n","      <td>train</td>\n","      <td>1791</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>call</td>\n","      <td>NN</td>\n","      <td>O</td>\n","      <td>train</td>\n","      <td>7320</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>to</td>\n","      <td>TO</td>\n","      <td>O</td>\n","      <td>train</td>\n","      <td>21011</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #     Word  POS     Tag  Split  Word_idx  Tag_idx\n","0  Sentence: 1       EU  NNP   B-ORG  train      7450        1\n","1  Sentence: 1  rejects  VBZ       O  train     12056        0\n","2  Sentence: 1   German   JJ  B-MISC  train      1791        6\n","3  Sentence: 1     call   NN       O  train      7320        0\n","4  Sentence: 1       to   TO       O  train     21011        0"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Add index info to dataframe\n","data['Word_idx'] = data['Word'].map(token2idx)\n","data['Tag_idx'] = data['Tag'].map(tag2idx)\n","data.head()"]},{"cell_type":"code","execution_count":22,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:28:59.168683600Z","start_time":"2023-12-10T14:28:56.677269900Z"},"execution":{"iopub.execute_input":"2023-12-10T14:41:13.276819Z","iopub.status.busy":"2023-12-10T14:41:13.276411Z","iopub.status.idle":"2023-12-10T14:41:17.154255Z","shell.execute_reply":"2023-12-10T14:41:17.153311Z","shell.execute_reply.started":"2023-12-10T14:41:13.276778Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_42/3280286815.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  data_fillna = data.fillna(method='ffill', axis=0)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>Word_idx</th>\n","      <th>Tag_idx</th>\n","      <th>Split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n","      <td>[NNP, VBZ, JJ, NN, TO, VB, JJ, NN, .]</td>\n","      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n","      <td>[7450, 12056, 1791, 7320, 21011, 12400, 25419,...</td>\n","      <td>[1, 0, 6, 0, 0, 0, 6, 0, 0]</td>\n","      <td>[train, train, train, train, train, train, tra...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 10</td>\n","      <td>[But, Fischler, agreed, to, review, his, propo...</td>\n","      <td>[CC, NNP, VBD, TO, VB, PRP$, NN, IN, DT, NNP, ...</td>\n","      <td>[O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...</td>\n","      <td>[16088, 10397, 1878, 21011, 16625, 4853, 4421,...</td>\n","      <td>[0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n","      <td>[train, train, train, train, train, train, tra...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 100</td>\n","      <td>[The, Syrians, are, confused, ,, they, are, de...</td>\n","      <td>[DT, NNPS, VBP, VBN, ,, PRP, VBP, RB, JJ, ,, C...</td>\n","      <td>[O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n","      <td>[15206, 6558, 18695, 2236, 26419, 12948, 18695...</td>\n","      <td>[0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[train, train, train, train, train, train, tra...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1000</td>\n","      <td>[The, youth, side, replied, with, 246, for, se...</td>\n","      <td>[DT, NN, NN, VBD, IN, CD, IN, CD, .]</td>\n","      <td>[O, O, O, O, O, O, O, O, O]</td>\n","      <td>[15206, 8435, 8052, 13448, 19470, 11187, 5326,...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","      <td>[train, train, train, train, train, train, tra...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 10000</td>\n","      <td>[Men, 's, 3,000, metres, :]</td>\n","      <td>[NN, POS, CD, NNS, :]</td>\n","      <td>[O, O, O, O, O]</td>\n","      <td>[9988, 23683, 2082, 19984, 16435]</td>\n","      <td>[0, 0, 0, 0, 0]</td>\n","      <td>[train, train, train, train, train]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Sentence #                                               Word  \\\n","0      Sentence: 1  [EU, rejects, German, call, to, boycott, Briti...   \n","1     Sentence: 10  [But, Fischler, agreed, to, review, his, propo...   \n","2    Sentence: 100  [The, Syrians, are, confused, ,, they, are, de...   \n","3   Sentence: 1000  [The, youth, side, replied, with, 246, for, se...   \n","4  Sentence: 10000                        [Men, 's, 3,000, metres, :]   \n","\n","                                                 POS  \\\n","0              [NNP, VBZ, JJ, NN, TO, VB, JJ, NN, .]   \n","1  [CC, NNP, VBD, TO, VB, PRP$, NN, IN, DT, NNP, ...   \n","2  [DT, NNPS, VBP, VBN, ,, PRP, VBP, RB, JJ, ,, C...   \n","3               [DT, NN, NN, VBD, IN, CD, IN, CD, .]   \n","4                              [NN, POS, CD, NNS, :]   \n","\n","                                                 Tag  \\\n","0          [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]   \n","1  [O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...   \n","2  [O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...   \n","3                        [O, O, O, O, O, O, O, O, O]   \n","4                                    [O, O, O, O, O]   \n","\n","                                            Word_idx  \\\n","0  [7450, 12056, 1791, 7320, 21011, 12400, 25419,...   \n","1  [16088, 10397, 1878, 21011, 16625, 4853, 4421,...   \n","2  [15206, 6558, 18695, 2236, 26419, 12948, 18695...   \n","3  [15206, 8435, 8052, 13448, 19470, 11187, 5326,...   \n","4                  [9988, 23683, 2082, 19984, 16435]   \n","\n","                                             Tag_idx  \\\n","0                        [1, 0, 6, 0, 0, 0, 6, 0, 0]   \n","1  [0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n","2  [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","3                        [0, 0, 0, 0, 0, 0, 0, 0, 0]   \n","4                                    [0, 0, 0, 0, 0]   \n","\n","                                               Split  \n","0  [train, train, train, train, train, train, tra...  \n","1  [train, train, train, train, train, train, tra...  \n","2  [train, train, train, train, train, train, tra...  \n","3  [train, train, train, train, train, train, tra...  \n","4                [train, train, train, train, train]  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Group data by sentences\n","# Fill na\n","data_fillna = data.fillna(method='ffill', axis=0)\n","# Groupby and collect columns\n","data_group = data_fillna.groupby(['Sentence #'], as_index=False)[['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx', 'Split']].agg(lambda x: list(x))\n","# Visualise data\n","data_group.head()"]},{"cell_type":"code","execution_count":23,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:18.584852300Z","start_time":"2023-12-10T14:29:17.074940500Z"},"execution":{"iopub.execute_input":"2023-12-10T14:41:19.382354Z","iopub.status.busy":"2023-12-10T14:41:19.381892Z","iopub.status.idle":"2023-12-10T14:41:20.876702Z","shell.execute_reply":"2023-12-10T14:41:20.875434Z","shell.execute_reply.started":"2023-12-10T14:41:19.382316Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["27316\n","padding 124\n","train_tokens length: 14041 \n","train_tokens length: 14041 \n","val_tokens: 3453 \n","val_tags: 3453\n"]}],"source":["# Change eval_split from 'dev' to test to run on test data\n","def get_pad_train_test_val(data_group, data, eval_split='dev', n_vocab = n_vocab):\n","\n","    #get max token and tag length\n","    n_token = len(list(set(data['Word'].to_list())))\n","    n_tag = len(list(set(data['Tag'].to_list())))\n","    print(n_token)\n","\n","    #Pad tokens (X var)    \n","    tokens = data_group['Word_idx'].tolist()\n","    maxlen = max([len(s) for s in tokens])\n","    # value should be the number of items in the vocb?\n","    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int64', padding='post', value= n_vocab)\n","    print('padding', len(pad_tokens[0]))\n","    # I used the code below to check the if the padded vectors are set to 0:\n","#     for token in pad_tokens:\n","#         print(token[-1])\n","# #         print(embedding_matrix[token[-1]])\n","#         break\n","\n","    #Pad Tags (y var) and convert it into one hot encoding\n","    tags = data_group['Tag_idx'].tolist()\n","    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int64', padding='post', value= tag2idx[\"O\"])\n","    n_tags = len(tag2idx)\n","    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n","    \n","    train_tokens = []\n","    dev_tokens = []\n","    train_tags = []\n","    dev_tags = []\n","    for i, row in data_group.iterrows():\n","        if 'train' in row['Split']:\n","            train_tokens.append(pad_tokens[i])\n","            train_tags.append(pad_tags[i])\n","        elif eval_split in row['Split']:\n","            #dev_idx.append(i)\n","            dev_tokens.append(pad_tokens[i])\n","            dev_tags.append(pad_tags[i])\n","\n","    print(\n","        'train_tokens length:', len(train_tokens),\n","        '\\ntrain_tokens length:', len(train_tokens),\n","        #'\\ntest_tokens length:', len(test_tokens),\n","        #'\\ntest_tags:', len(test_tags),\n","        '\\nval_tokens:', len(dev_tokens),\n","        '\\nval_tags:', len(dev_tags))\n"," \n","    return np.array(train_tokens), np.array(dev_tokens),  np.array(train_tags), np.array(dev_tags)\n","\n","train_tokens, dev_tokens,  train_tags, dev_tags = get_pad_train_test_val(data_group, data, eval_split= eval_split)"]},{"cell_type":"markdown","metadata":{},"source":["# Build model"]},{"cell_type":"code","execution_count":25,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:20.955252800Z","start_time":"2023-12-10T14:29:20.922279100Z"},"execution":{"iopub.execute_input":"2023-12-10T14:41:30.349486Z","iopub.status.busy":"2023-12-10T14:41:30.349119Z","iopub.status.idle":"2023-12-10T14:41:30.385309Z","shell.execute_reply":"2023-12-10T14:41:30.384303Z","shell.execute_reply.started":"2023-12-10T14:41:30.349455Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input_dim:  27317 \n","output_dim:  300 \n","input_length:  124 \n","n_tags:  9\n","emb dim 300\n"]}],"source":["input_dim = len(list(set(data['Word'].to_list()))) +1\n","output_dim = emb_dim # number of dimensions\n","input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n","n_tags = len(tag2idx)\n","print('input_dim: ', \n","      input_dim, '\\noutput_dim: ', \n","      output_dim, '\\ninput_length: ', \n","      input_length, '\\nn_tags: ', n_tags)\n","print('emb dim', emb_dim)"]},{"cell_type":"code","execution_count":26,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:22.052926300Z","start_time":"2023-12-10T14:29:22.043624300Z"},"execution":{"iopub.execute_input":"2023-12-10T14:41:30.925150Z","iopub.status.busy":"2023-12-10T14:41:30.924739Z","iopub.status.idle":"2023-12-10T14:41:30.932768Z","shell.execute_reply":"2023-12-10T14:41:30.931780Z","shell.execute_reply.started":"2023-12-10T14:41:30.925112Z"},"trusted":true},"outputs":[],"source":["def get_bilstm_lstm_model(embedding_matrix, embedding_dim):\n","    \n","    model = Sequential()\n","    #token2idx\n","    # Add Embedding layer original, trainable\n","    #model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n","    print(len(token2idx))\n","    embedding_layer = Embedding(len(token2idx)+1 ,\n","                            embedding_dim,\n","                            weights=[embedding_matrix],\n","                            # make max sent length a variable\n","                            input_length=input_length,\n","                            trainable=False)\n","    model.add(embedding_layer)\n","\n","    # Add bidirectional LSTM\n","    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n","\n","    # Add LSTM\n","    # Pia decided to remove this\n","#     model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n","\n","    # Add timeDistributed Layer\n","    # Pia: replaced relu with sigmoid \n","    model.add(TimeDistributed(Dense(n_tags, activation=\"sigmoid\")))\n","\n"," \n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n","    model.summary()\n","    \n","    return model"]},{"cell_type":"code","execution_count":31,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:23.791339200Z","start_time":"2023-12-10T14:29:23.780599100Z"},"execution":{"iopub.execute_input":"2023-12-10T15:01:29.595660Z","iopub.status.busy":"2023-12-10T15:01:29.595260Z","iopub.status.idle":"2023-12-10T15:01:29.602011Z","shell.execute_reply":"2023-12-10T15:01:29.600837Z","shell.execute_reply.started":"2023-12-10T15:01:29.595632Z"},"trusted":true},"outputs":[],"source":["def train_model(X, y, model):\n","    loss = list()\n","    for i in range(20):\n","        # fit model for one epoch on this sequence\n","        hist = model.fit(X, y, batch_size=200, verbose=1, epochs=1, validation_split=0.2)\n","        loss.append(hist.history['loss'][0])\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["# Train model "]},{"cell_type":"code","execution_count":32,"metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:30:46.193236500Z","start_time":"2023-12-10T14:29:30.699744700Z"},"execution":{"iopub.execute_input":"2023-12-10T15:01:30.533009Z","iopub.status.busy":"2023-12-10T15:01:30.532633Z","iopub.status.idle":"2023-12-10T15:20:15.770246Z","shell.execute_reply":"2023-12-10T15:20:15.769284Z","shell.execute_reply.started":"2023-12-10T15:01:30.532978Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["27316\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 124, 300)          8195100   \n","                                                                 \n"," bidirectional_2 (Bidirecti  (None, 124, 600)          1442400   \n"," onal)                                                           \n","                                                                 \n"," time_distributed_2 (TimeDi  (None, 124, 9)            5409      \n"," stributed)                                                      \n","                                                                 \n","=================================================================\n","Total params: 9642909 (36.78 MB)\n","Trainable params: 1447809 (5.52 MB)\n","Non-trainable params: 8195100 (31.26 MB)\n","_________________________________________________________________\n","57/57 [==============================] - 48s 743ms/step - loss: 0.2941 - accuracy: 0.9691 - val_loss: 0.1012 - val_accuracy: 0.9817\n","57/57 [==============================] - 42s 731ms/step - loss: 0.1034 - accuracy: 0.9800 - val_loss: 0.0928 - val_accuracy: 0.9817\n","57/57 [==============================] - 42s 734ms/step - loss: 0.0938 - accuracy: 0.9801 - val_loss: 0.0876 - val_accuracy: 0.9817\n","57/57 [==============================] - 42s 740ms/step - loss: 0.0895 - accuracy: 0.9801 - val_loss: 0.0808 - val_accuracy: 0.9817\n","57/57 [==============================] - 42s 729ms/step - loss: 0.0805 - accuracy: 0.9801 - val_loss: 0.0735 - val_accuracy: 0.9817\n","57/57 [==============================] - 41s 726ms/step - loss: 0.0766 - accuracy: 0.9801 - val_loss: 0.0679 - val_accuracy: 0.9817\n","57/57 [==============================] - 42s 744ms/step - loss: 0.0683 - accuracy: 0.9801 - val_loss: 0.0633 - val_accuracy: 0.9817\n","57/57 [==============================] - 41s 726ms/step - loss: 0.0630 - accuracy: 0.9801 - val_loss: 0.0648 - val_accuracy: 0.9817\n","57/57 [==============================] - 41s 728ms/step - loss: 0.0582 - accuracy: 0.9802 - val_loss: 0.0542 - val_accuracy: 0.9823\n","57/57 [==============================] - 41s 725ms/step - loss: 0.0506 - accuracy: 0.9846 - val_loss: 0.0455 - val_accuracy: 0.9880\n","57/57 [==============================] - 41s 728ms/step - loss: 0.0446 - accuracy: 0.9869 - val_loss: 0.0423 - val_accuracy: 0.9891\n","57/57 [==============================] - 42s 740ms/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 0.0366 - val_accuracy: 0.9904\n","57/57 [==============================] - 42s 737ms/step - loss: 0.0371 - accuracy: 0.9896 - val_loss: 0.0343 - val_accuracy: 0.9900\n","57/57 [==============================] - 41s 727ms/step - loss: 0.0347 - accuracy: 0.9905 - val_loss: 0.0370 - val_accuracy: 0.9912\n","57/57 [==============================] - 42s 730ms/step - loss: 0.0331 - accuracy: 0.9911 - val_loss: 0.0316 - val_accuracy: 0.9914\n","57/57 [==============================] - 42s 729ms/step - loss: 0.0316 - accuracy: 0.9916 - val_loss: 0.0304 - val_accuracy: 0.9917\n","57/57 [==============================] - 42s 736ms/step - loss: 0.0303 - accuracy: 0.9920 - val_loss: 0.0329 - val_accuracy: 0.9917\n","57/57 [==============================] - 42s 733ms/step - loss: 0.0293 - accuracy: 0.9922 - val_loss: 0.0285 - val_accuracy: 0.9924\n","57/57 [==============================] - 41s 726ms/step - loss: 0.0285 - accuracy: 0.9924 - val_loss: 0.0279 - val_accuracy: 0.9924\n","57/57 [==============================] - 42s 736ms/step - loss: 0.0277 - accuracy: 0.9926 - val_loss: 0.0312 - val_accuracy: 0.9914\n"]}],"source":["results = pd.DataFrame()\n","embedding_dim = 300 # dimensions of the word2vec vectors\n","model_bilstm_lstm = get_bilstm_lstm_model(embedding_matrix, embedding_dim)\n","plot_model(model_bilstm_lstm)\n","# change to val_tokens to try out training on val set\n","results['with_add_lstm'] = train_model(train_tokens, train_tags, model_bilstm_lstm)"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate\n","\n","The code below evaluates your model on the development data using accuracy (which is not very indicative on this task. To get better insights, store the model output and run your own evaluation."]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T15:20:15.773026Z","iopub.status.busy":"2023-12-10T15:20:15.772358Z","iopub.status.idle":"2023-12-10T15:23:51.800593Z","shell.execute_reply":"2023-12-10T15:23:51.799601Z","shell.execute_reply.started":"2023-12-10T15:20:15.772979Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluate on test data\n","3453/3453 [==============================] - 216s 63ms/step - loss: 0.0317 - accuracy: 0.9916\n","test loss, test acc: [0.031662166118621826, 0.9916412234306335]\n"]}],"source":["# Evaluate the model on the test data using `evaluate`\n","# Careful: Really high even if the model only predicts the majority class\n","\n","results = model_bilstm_lstm.evaluate(dev_tokens, np.array(dev_tags), batch_size=1)"]},{"cell_type":"markdown","metadata":{},"source":["# Get model predictions"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T15:23:51.809414Z","iopub.status.busy":"2023-12-10T15:23:51.809146Z","iopub.status.idle":"2023-12-10T15:23:58.238117Z","shell.execute_reply":"2023-12-10T15:23:58.237283Z","shell.execute_reply.started":"2023-12-10T15:23:51.809383Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["108/108 [==============================] - 6s 54ms/step\n"]}],"source":["# Get predictions on development set\n","y_pred = model_bilstm_lstm.predict(dev_tokens)\n","\n","# get dimension index with highest prob (--> label)\n","y_pred = np.argmax(y_pred, axis=-1)\n","y_dev =  np.argmax(dev_tags, axis=-1)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T15:35:07.299530Z","iopub.status.busy":"2023-12-10T15:35:07.298748Z","iopub.status.idle":"2023-12-10T15:35:07.501864Z","shell.execute_reply":"2023-12-10T15:35:07.500817Z","shell.execute_reply.started":"2023-12-10T15:35:07.299499Z"},"trusted":true},"outputs":[],"source":["# Get predictions per token:\n","# map labels back to tokens\n","\n","def output_to_file(dev_tokens, y_pred, output_path):\n","    \n","    with open(output_path, 'w') as outfile:\n","        for token,  preds in zip(dev_tokens, y_pred):\n","            for tok, pred in zip(token, preds):\n","                # igonre padding:\n","                if tok in idx2token:\n","                    tok_str = idx2token[tok]\n","                    outfile.write(f'{tok_str}\\t{idx2tag[pred]}\\n')\n","    \n","output_to_file(dev_tokens, y_pred, \"lstmout.txt\")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T15:24:52.684210Z","iopub.status.busy":"2023-12-10T15:24:52.683769Z","iopub.status.idle":"2023-12-10T15:24:52.944497Z","shell.execute_reply":"2023-12-10T15:24:52.943414Z","shell.execute_reply.started":"2023-12-10T15:24:52.684176Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open(\"lstm.pkl\", 'wb') as f:\n","        pickle.dump(model_bilstm_lstm, f)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4141680,"sourceId":7168852,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
