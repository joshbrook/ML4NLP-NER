{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7168852,"sourceType":"datasetVersion","datasetId":4141680}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LSTM for NER\n\nAdapted from [this](https://github.com/cltl/ma-ml4nlp-labs/blob/main/code/assignment3/lstm-ner.ipynb) notebook","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow\nfrom tensorflow.keras.optimizers import Adam\n\nfrom keras import Sequential, Model, Input, optimizers\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom keras.utils import plot_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom itertools import chain\n\nfrom gensim.models import KeyedVectors\nimport gensim.downloader as api","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:12:39.176440300Z","start_time":"2023-12-10T14:12:39.132674Z"},"execution":{"iopub.status.busy":"2023-12-13T12:16:37.071043Z","iopub.execute_input":"2023-12-13T12:16:37.071423Z","iopub.status.idle":"2023-12-13T12:17:07.589357Z","shell.execute_reply.started":"2023-12-13T12:16:37.071394Z","shell.execute_reply":"2023-12-13T12:17:07.588362Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Set path to data and embeddings:","metadata":{}},{"cell_type":"code","source":"# conll data\npath_train ='/kaggle/input/ner-data/conll2003.train.conll'\npath_eval = '/kaggle/input/ner-data/conll2003.test.conll'\n\npaths = [path_train, path_eval]\n\n# change to test if you are evaluating on test:\neval_split = 'test'\n\n# model output path\noutput_path = 'lstm-out.csv'","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:15:36.421358600Z","start_time":"2023-12-10T14:15:36.409126300Z"},"execution":{"iopub.status.busy":"2023-12-13T12:17:07.591234Z","iopub.execute_input":"2023-12-13T12:17:07.591751Z","iopub.status.idle":"2023-12-13T12:17:07.596509Z","shell.execute_reply.started":"2023-12-13T12:17:07.591727Z","shell.execute_reply":"2023-12-13T12:17:07.595580Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"def convert_data(paths):\n    \n    data = []\n    sent_id = 1\n    for path in paths:\n        split = path.split('.')[-2]\n        with open(path) as infile:\n            lines = infile.read().split('\\n')\n        for n, line in enumerate(lines):\n            ll = line.split('\\t')\n            if len(ll) > 2:\n                d = dict()\n                d['Sentence #'] = f'Sentence: {sent_id}'\n                d['Word'] = ll[0]\n                d['POS'] = ll[1]\n                d['Tag'] = ll[-1]\n                d['Split'] = split\n                data.append(d)\n\n            else:\n                sent_id += 1\n    data = pd.DataFrame(data)\n    return data\n\ndata = convert_data(paths)","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:15:39.360913800Z","start_time":"2023-12-10T14:15:38.834486700Z"},"execution":{"iopub.status.busy":"2023-12-13T12:17:07.597871Z","iopub.execute_input":"2023-12-13T12:17:07.598525Z","iopub.status.idle":"2023-12-13T12:17:08.459832Z","shell.execute_reply.started":"2023-12-13T12:17:07.598499Z","shell.execute_reply":"2023-12-13T12:17:08.458854Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Map tokens and labels to indices","metadata":{}},{"cell_type":"code","source":"def get_dict_map(data, token_or_tag, embedding_model=None):\n    \"\"\"map tokens and labels to indices\"\"\"\n    tok2idx = {}\n    idx2tok = {}\n    \n    if token_or_tag == 'token':\n        vocab = list(set(data['Word'].to_list()))\n    else:\n        vocab = list(set(data['Tag'].to_list()))\n    \n    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}   \n    \n    return tok2idx, idx2tok\n\n\ntoken2idx, idx2token = get_dict_map(data, 'token')\ntag2idx, idx2tag = get_dict_map(data, 'tag')\nn_vocab = len(token2idx)\nn_tags = len(tag2idx)\nprint(n_vocab)\nprint(n_tags)","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:15:51.085074100Z","start_time":"2023-12-10T14:15:51.010005100Z"},"execution":{"iopub.status.busy":"2023-12-13T12:17:08.462513Z","iopub.execute_input":"2023-12-13T12:17:08.462889Z","iopub.status.idle":"2023-12-13T12:17:08.531258Z","shell.execute_reply.started":"2023-12-13T12:17:08.462856Z","shell.execute_reply":"2023-12-13T12:17:08.530120Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"27316\n9\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Integrating embeddings","metadata":{}},{"cell_type":"code","source":"# Load embedding model\nw2v_model = api.load('word2vec-google-news-300')\n\n# Create embedding matrix with zero vectors for oov words\nemb_dim = 300\nembedding_matrix = np.zeros((len(token2idx) + 1, emb_dim))\nprint(embedding_matrix.shape)\nfor word, i in token2idx.items():\n    # You may have to change the following line to:\n    # if word in w2v_model:\n    if word in w2v_model.key_to_index:\n        embedding_vector = w2v_model[word]\n    else:\n        embedding_vector = None\n        # If you want to check OOV words:\n        #print('couldnt find:', word, i)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nemb_dim = embedding_matrix.shape[1]","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:16:37.599930500Z","start_time":"2023-12-10T14:15:53.018954Z"},"execution":{"iopub.status.busy":"2023-12-13T12:17:08.532568Z","iopub.execute_input":"2023-12-13T12:17:08.532956Z","iopub.status.idle":"2023-12-13T12:21:27.059508Z","shell.execute_reply.started":"2023-12-13T12:17:08.532921Z","shell.execute_reply":"2023-12-13T12:21:27.058603Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[=================---------------------------------] 34.7% 576.6/1662.8MB downloaded(27317, 300)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Add index info to dataframe\ndata['Word_idx'] = data['Word'].map(token2idx)\ndata['Tag_idx'] = data['Tag'].map(tag2idx)\ndata.head()","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:16:47.803719100Z","start_time":"2023-12-10T14:16:47.672121700Z"},"execution":{"iopub.status.busy":"2023-12-13T12:21:27.060611Z","iopub.execute_input":"2023-12-13T12:21:27.060938Z","iopub.status.idle":"2023-12-13T12:21:27.226365Z","shell.execute_reply.started":"2023-12-13T12:21:27.060911Z","shell.execute_reply":"2023-12-13T12:21:27.225434Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    Sentence #     Word  POS     Tag  Split  Word_idx  Tag_idx\n0  Sentence: 1       EU  NNP   B-ORG  train      7000        8\n1  Sentence: 1  rejects  VBZ       O  train     23361        5\n2  Sentence: 1   German   JJ  B-MISC  train      2299        2\n3  Sentence: 1     call   NN       O  train     10140        5\n4  Sentence: 1       to   TO       O  train      5701        5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n      <th>Split</th>\n      <th>Word_idx</th>\n      <th>Tag_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>EU</td>\n      <td>NNP</td>\n      <td>B-ORG</td>\n      <td>train</td>\n      <td>7000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence: 1</td>\n      <td>rejects</td>\n      <td>VBZ</td>\n      <td>O</td>\n      <td>train</td>\n      <td>23361</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sentence: 1</td>\n      <td>German</td>\n      <td>JJ</td>\n      <td>B-MISC</td>\n      <td>train</td>\n      <td>2299</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sentence: 1</td>\n      <td>call</td>\n      <td>NN</td>\n      <td>O</td>\n      <td>train</td>\n      <td>10140</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sentence: 1</td>\n      <td>to</td>\n      <td>TO</td>\n      <td>O</td>\n      <td>train</td>\n      <td>5701</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Group data by sentences\n# Fill na\ndata_fillna = data.fillna(method='ffill', axis=0)\n# Groupby and collect columns\ndata_group = data_fillna.groupby(['Sentence #'], as_index=False)[['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx', 'Split']].agg(lambda x: list(x))\n# Visualise data\ndata_group.head()","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:28:59.168683600Z","start_time":"2023-12-10T14:28:56.677269900Z"},"execution":{"iopub.status.busy":"2023-12-13T12:21:27.227446Z","iopub.execute_input":"2023-12-13T12:21:27.227788Z","iopub.status.idle":"2023-12-13T12:21:31.064573Z","shell.execute_reply.started":"2023-12-13T12:21:27.227762Z","shell.execute_reply":"2023-12-13T12:21:31.063508Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_42/3280286815.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data_fillna = data.fillna(method='ffill', axis=0)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        Sentence #                                               Word  \\\n0      Sentence: 1  [EU, rejects, German, call, to, boycott, Briti...   \n1     Sentence: 10  [But, Fischler, agreed, to, review, his, propo...   \n2    Sentence: 100  [The, Syrians, are, confused, ,, they, are, de...   \n3   Sentence: 1000  [The, youth, side, replied, with, 246, for, se...   \n4  Sentence: 10000                        [Men, 's, 3,000, metres, :]   \n\n                                                 POS  \\\n0              [NNP, VBZ, JJ, NN, TO, VB, JJ, NN, .]   \n1  [CC, NNP, VBD, TO, VB, PRP$, NN, IN, DT, NNP, ...   \n2  [DT, NNPS, VBP, VBN, ,, PRP, VBP, RB, JJ, ,, C...   \n3               [DT, NN, NN, VBD, IN, CD, IN, CD, .]   \n4                              [NN, POS, CD, NNS, :]   \n\n                                                 Tag  \\\n0          [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]   \n1  [O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...   \n2  [O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...   \n3                        [O, O, O, O, O, O, O, O, O]   \n4                                    [O, O, O, O, O]   \n\n                                            Word_idx  \\\n0  [7000, 23361, 2299, 10140, 5701, 9662, 23069, ...   \n1  [20538, 1904, 24299, 5701, 5242, 8100, 2219, 2...   \n2  [5384, 5002, 5849, 12843, 9539, 13728, 5849, 2...   \n3  [5384, 18446, 22083, 12351, 26467, 7685, 15985...   \n4                  [20810, 18978, 6112, 6805, 14973]   \n\n                                             Tag_idx  \\\n0                        [8, 5, 2, 5, 5, 5, 2, 5, 5]   \n1  [5, 6, 5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 5, 5, 5, ...   \n2  [5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n3                        [5, 5, 5, 5, 5, 5, 5, 5, 5]   \n4                                    [5, 5, 5, 5, 5]   \n\n                                               Split  \n0  [train, train, train, train, train, train, tra...  \n1  [train, train, train, train, train, train, tra...  \n2  [train, train, train, train, train, train, tra...  \n3  [train, train, train, train, train, train, tra...  \n4                [train, train, train, train, train]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n      <th>Word_idx</th>\n      <th>Tag_idx</th>\n      <th>Split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n      <td>[NNP, VBZ, JJ, NN, TO, VB, JJ, NN, .]</td>\n      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n      <td>[7000, 23361, 2299, 10140, 5701, 9662, 23069, ...</td>\n      <td>[8, 5, 2, 5, 5, 5, 2, 5, 5]</td>\n      <td>[train, train, train, train, train, train, tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence: 10</td>\n      <td>[But, Fischler, agreed, to, review, his, propo...</td>\n      <td>[CC, NNP, VBD, TO, VB, PRP$, NN, IN, DT, NNP, ...</td>\n      <td>[O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...</td>\n      <td>[20538, 1904, 24299, 5701, 5242, 8100, 2219, 2...</td>\n      <td>[5, 6, 5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 5, 5, 5, ...</td>\n      <td>[train, train, train, train, train, train, tra...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sentence: 100</td>\n      <td>[The, Syrians, are, confused, ,, they, are, de...</td>\n      <td>[DT, NNPS, VBP, VBN, ,, PRP, VBP, RB, JJ, ,, C...</td>\n      <td>[O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n      <td>[5384, 5002, 5849, 12843, 9539, 13728, 5849, 2...</td>\n      <td>[5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n      <td>[train, train, train, train, train, train, tra...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sentence: 1000</td>\n      <td>[The, youth, side, replied, with, 246, for, se...</td>\n      <td>[DT, NN, NN, VBD, IN, CD, IN, CD, .]</td>\n      <td>[O, O, O, O, O, O, O, O, O]</td>\n      <td>[5384, 18446, 22083, 12351, 26467, 7685, 15985...</td>\n      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5]</td>\n      <td>[train, train, train, train, train, train, tra...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sentence: 10000</td>\n      <td>[Men, 's, 3,000, metres, :]</td>\n      <td>[NN, POS, CD, NNS, :]</td>\n      <td>[O, O, O, O, O]</td>\n      <td>[20810, 18978, 6112, 6805, 14973]</td>\n      <td>[5, 5, 5, 5, 5]</td>\n      <td>[train, train, train, train, train]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Change eval_split from 'dev' to test to run on test data\ndef get_pad_train_test_val(data_group, data, eval_split='dev', n_vocab = n_vocab):\n\n    #get max token and tag length\n    n_token = len(list(set(data['Word'].to_list())))\n    n_tag = len(list(set(data['Tag'].to_list())))\n    print(n_token)\n\n    #Pad tokens (X var)    \n    tokens = data_group['Word_idx'].tolist()\n    maxlen = max([len(s) for s in tokens])\n    # value should be the number of items in the vocb?\n    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int64', padding='post', value= n_vocab)\n    print('padding', len(pad_tokens[0]))\n    # I used the code below to check the if the padded vectors are set to 0:\n#     for token in pad_tokens:\n#         print(token[-1])\n# #         print(embedding_matrix[token[-1]])\n#         break\n\n    #Pad Tags (y var) and convert it into one hot encoding\n    tags = data_group['Tag_idx'].tolist()\n    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int64', padding='post', value= tag2idx[\"O\"])\n    n_tags = len(tag2idx)\n    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n    \n    train_tokens = []\n    dev_tokens = []\n    train_tags = []\n    dev_tags = []\n    for i, row in data_group.iterrows():\n        if 'train' in row['Split']:\n            train_tokens.append(pad_tokens[i])\n            train_tags.append(pad_tags[i])\n        elif eval_split in row['Split']:\n            #dev_idx.append(i)\n            dev_tokens.append(pad_tokens[i])\n            dev_tags.append(pad_tags[i])\n\n    print(\n        'train_tokens length:', len(train_tokens),\n        '\\ntrain_tokens length:', len(train_tokens),\n        #'\\ntest_tokens length:', len(test_tokens),\n        #'\\ntest_tags:', len(test_tags),\n        '\\nval_tokens:', len(dev_tokens),\n        '\\nval_tags:', len(dev_tags))\n \n    return np.array(train_tokens), np.array(dev_tokens),  np.array(train_tags), np.array(dev_tags)\n\ntrain_tokens, dev_tokens,  train_tags, dev_tags = get_pad_train_test_val(data_group, data, eval_split= eval_split)","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:18.584852300Z","start_time":"2023-12-10T14:29:17.074940500Z"},"execution":{"iopub.status.busy":"2023-12-13T12:21:31.066017Z","iopub.execute_input":"2023-12-13T12:21:31.066332Z","iopub.status.idle":"2023-12-13T12:21:32.643077Z","shell.execute_reply.started":"2023-12-13T12:21:31.066306Z","shell.execute_reply":"2023-12-13T12:21:32.642137Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"27316\npadding 124\ntrain_tokens length: 14041 \ntrain_tokens length: 14041 \nval_tokens: 3453 \nval_tags: 3453\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Build model","metadata":{}},{"cell_type":"code","source":"input_dim = len(list(set(data['Word'].to_list()))) +1\noutput_dim = emb_dim # number of dimensions\ninput_length = max([len(s) for s in data_group['Word_idx'].tolist()])\nn_tags = len(tag2idx)\nprint('input_dim: ', \n      input_dim, '\\noutput_dim: ', \n      output_dim, '\\ninput_length: ', \n      input_length, '\\nn_tags: ', n_tags)\nprint('emb dim', emb_dim)","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:20.955252800Z","start_time":"2023-12-10T14:29:20.922279100Z"},"execution":{"iopub.status.busy":"2023-12-13T12:21:32.644476Z","iopub.execute_input":"2023-12-13T12:21:32.644834Z","iopub.status.idle":"2023-12-13T12:21:32.680396Z","shell.execute_reply.started":"2023-12-13T12:21:32.644804Z","shell.execute_reply":"2023-12-13T12:21:32.679357Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"input_dim:  27317 \noutput_dim:  300 \ninput_length:  124 \nn_tags:  9\nemb dim 300\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef f1(y_true, y_pred):    \n    def recall_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        \n        recall = TP / (Positives+K.epsilon())    \n        return recall \n    \n    \n    def precision_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n        precision = TP / (Pred_Positives+K.epsilon())\n        return precision \n    \n    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:21:32.683287Z","iopub.execute_input":"2023-12-13T12:21:32.683615Z","iopub.status.idle":"2023-12-13T12:21:32.693660Z","shell.execute_reply.started":"2023-12-13T12:21:32.683569Z","shell.execute_reply":"2023-12-13T12:21:32.692904Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_bilstm_lstm_model(embedding_matrix, embedding_dim):\n    \n    model = Sequential()\n    #token2idx\n    # Add Embedding layer original, trainable\n    #model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n\n    embedding_layer = Embedding(len(token2idx)+1 ,\n                            embedding_dim,\n                            weights=[embedding_matrix],\n                            # make max sent length a variable\n                            input_length=input_length,\n                            trainable=False)\n    model.add(embedding_layer)\n\n    # Add bidirectional LSTM\n    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n\n    # Add LSTM\n    # Pia decided to remove this\n    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n\n    # Add timeDistributed Layer\n    # Pia: replaced relu with sigmoid \n    model.add(TimeDistributed(Dense(n_tags, activation=\"sigmoid\")))\n \n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=[\"accuracy\", f1])\n    model.summary()\n    \n    return model","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:22.052926300Z","start_time":"2023-12-10T14:29:22.043624300Z"},"execution":{"iopub.status.busy":"2023-12-13T12:21:32.694722Z","iopub.execute_input":"2023-12-13T12:21:32.695126Z","iopub.status.idle":"2023-12-13T12:21:32.708076Z","shell.execute_reply.started":"2023-12-13T12:21:32.695074Z","shell.execute_reply":"2023-12-13T12:21:32.707066Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def train_model(X, y, model):\n    loss = list()\n    for i in range(30):\n        # fit model for one epoch on this sequence\n        print(\"Epoch:\", i)\n        hist = model.fit(X, y, batch_size=200, verbose=1, epochs=1, validation_split=0.2)\n        loss.append(hist.history['loss'][0])\n    return loss","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:29:23.791339200Z","start_time":"2023-12-10T14:29:23.780599100Z"},"execution":{"iopub.status.busy":"2023-12-13T12:21:32.709322Z","iopub.execute_input":"2023-12-13T12:21:32.709613Z","iopub.status.idle":"2023-12-13T12:21:32.726107Z","shell.execute_reply.started":"2023-12-13T12:21:32.709589Z","shell.execute_reply":"2023-12-13T12:21:32.725041Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Train model ","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame()\nembedding_dim = 300 # dimensions of the word2vec vectors\nmodel_bilstm_lstm = get_bilstm_lstm_model(embedding_matrix, embedding_dim)\nplot_model(model_bilstm_lstm)\n# change to val_tokens to try out training on val set\nresults['with_add_lstm'] = train_model(train_tokens, train_tags, model_bilstm_lstm)","metadata":{"ExecuteTime":{"end_time":"2023-12-10T14:30:46.193236500Z","start_time":"2023-12-10T14:29:30.699744700Z"},"execution":{"iopub.status.busy":"2023-12-13T12:21:32.727240Z","iopub.execute_input":"2023-12-13T12:21:32.728919Z","iopub.status.idle":"2023-12-13T13:11:40.475639Z","shell.execute_reply.started":"2023-12-13T12:21:32.728886Z","shell.execute_reply":"2023-12-13T13:11:40.474542Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 124, 300)          8195100   \n                                                                 \n bidirectional (Bidirection  (None, 124, 600)          1442400   \n al)                                                             \n                                                                 \n lstm_1 (LSTM)               (None, 124, 300)          1081200   \n                                                                 \n time_distributed (TimeDist  (None, 124, 9)            2709      \n ributed)                                                        \n                                                                 \n=================================================================\nTotal params: 10721409 (40.90 MB)\nTrainable params: 2526309 (9.64 MB)\nNon-trainable params: 8195100 (31.26 MB)\n_________________________________________________________________\n57/57 [==============================] - 83s 1s/step - loss: 0.3175 - accuracy: 0.9634 - f1: 0.9455 - val_loss: 0.1030 - val_accuracy: 0.9817 - val_f1: 0.9816\n57/57 [==============================] - 69s 1s/step - loss: 0.1041 - accuracy: 0.9801 - f1: 0.9801 - val_loss: 0.0957 - val_accuracy: 0.9817 - val_f1: 0.9816\n57/57 [==============================] - 69s 1s/step - loss: 0.0954 - accuracy: 0.9801 - f1: 0.9802 - val_loss: 0.0844 - val_accuracy: 0.9817 - val_f1: 0.9816\n57/57 [==============================] - 70s 1s/step - loss: 0.0830 - accuracy: 0.9801 - f1: 0.9802 - val_loss: 0.0671 - val_accuracy: 0.9817 - val_f1: 0.9817\n57/57 [==============================] - 70s 1s/step - loss: 0.0618 - accuracy: 0.9801 - f1: 0.9807 - val_loss: 0.0493 - val_accuracy: 0.9817 - val_f1: 0.9832\n57/57 [==============================] - 70s 1s/step - loss: 0.0463 - accuracy: 0.9823 - f1: 0.9822 - val_loss: 0.0342 - val_accuracy: 0.9916 - val_f1: 0.9777\n57/57 [==============================] - 70s 1s/step - loss: 0.0306 - accuracy: 0.9922 - f1: 0.9573 - val_loss: 0.0254 - val_accuracy: 0.9931 - val_f1: 0.9512\n57/57 [==============================] - 70s 1s/step - loss: 0.0239 - accuracy: 0.9939 - f1: 0.9428 - val_loss: 0.0209 - val_accuracy: 0.9944 - val_f1: 0.9457\n57/57 [==============================] - 69s 1s/step - loss: 0.0201 - accuracy: 0.9948 - f1: 0.9442 - val_loss: 0.0187 - val_accuracy: 0.9950 - val_f1: 0.9413\n57/57 [==============================] - 69s 1s/step - loss: 0.0177 - accuracy: 0.9955 - f1: 0.9401 - val_loss: 0.0175 - val_accuracy: 0.9954 - val_f1: 0.9636\n57/57 [==============================] - 69s 1s/step - loss: 0.0160 - accuracy: 0.9959 - f1: 0.9430 - val_loss: 0.0169 - val_accuracy: 0.9955 - val_f1: 0.9268\n57/57 [==============================] - 69s 1s/step - loss: 0.0148 - accuracy: 0.9962 - f1: 0.9421 - val_loss: 0.0158 - val_accuracy: 0.9958 - val_f1: 0.9513\n57/57 [==============================] - 69s 1s/step - loss: 0.0139 - accuracy: 0.9964 - f1: 0.9451 - val_loss: 0.0161 - val_accuracy: 0.9955 - val_f1: 0.9290\n57/57 [==============================] - 69s 1s/step - loss: 0.0132 - accuracy: 0.9965 - f1: 0.9438 - val_loss: 0.0152 - val_accuracy: 0.9957 - val_f1: 0.9416\n57/57 [==============================] - 69s 1s/step - loss: 0.0125 - accuracy: 0.9967 - f1: 0.9440 - val_loss: 0.0144 - val_accuracy: 0.9960 - val_f1: 0.9580\n57/57 [==============================] - 70s 1s/step - loss: 0.0119 - accuracy: 0.9969 - f1: 0.9469 - val_loss: 0.0144 - val_accuracy: 0.9959 - val_f1: 0.9461\n57/57 [==============================] - 69s 1s/step - loss: 0.0114 - accuracy: 0.9970 - f1: 0.9447 - val_loss: 0.0142 - val_accuracy: 0.9960 - val_f1: 0.9439\n57/57 [==============================] - 69s 1s/step - loss: 0.0109 - accuracy: 0.9971 - f1: 0.9480 - val_loss: 0.0143 - val_accuracy: 0.9959 - val_f1: 0.9364\n57/57 [==============================] - 70s 1s/step - loss: 0.0103 - accuracy: 0.9972 - f1: 0.9490 - val_loss: 0.0146 - val_accuracy: 0.9959 - val_f1: 0.9302\n57/57 [==============================] - 70s 1s/step - loss: 0.0101 - accuracy: 0.9973 - f1: 0.9459 - val_loss: 0.0135 - val_accuracy: 0.9963 - val_f1: 0.9517\n57/57 [==============================] - 71s 1s/step - loss: 0.0096 - accuracy: 0.9974 - f1: 0.9477 - val_loss: 0.0139 - val_accuracy: 0.9960 - val_f1: 0.9413\n57/57 [==============================] - 70s 1s/step - loss: 0.0093 - accuracy: 0.9975 - f1: 0.9457 - val_loss: 0.0131 - val_accuracy: 0.9964 - val_f1: 0.9518\n57/57 [==============================] - 70s 1s/step - loss: 0.0088 - accuracy: 0.9976 - f1: 0.9525 - val_loss: 0.0135 - val_accuracy: 0.9962 - val_f1: 0.9453\n57/57 [==============================] - 70s 1s/step - loss: 0.0086 - accuracy: 0.9976 - f1: 0.9519 - val_loss: 0.0131 - val_accuracy: 0.9965 - val_f1: 0.9534\n57/57 [==============================] - 71s 1s/step - loss: 0.0082 - accuracy: 0.9978 - f1: 0.9515 - val_loss: 0.0128 - val_accuracy: 0.9966 - val_f1: 0.9610\n57/57 [==============================] - 70s 1s/step - loss: 0.0079 - accuracy: 0.9979 - f1: 0.9545 - val_loss: 0.0129 - val_accuracy: 0.9965 - val_f1: 0.9613\n57/57 [==============================] - 70s 1s/step - loss: 0.0077 - accuracy: 0.9979 - f1: 0.9541 - val_loss: 0.0130 - val_accuracy: 0.9966 - val_f1: 0.9669\n57/57 [==============================] - 69s 1s/step - loss: 0.0073 - accuracy: 0.9980 - f1: 0.9562 - val_loss: 0.0128 - val_accuracy: 0.9965 - val_f1: 0.9646\n57/57 [==============================] - 69s 1s/step - loss: 0.0071 - accuracy: 0.9980 - f1: 0.9544 - val_loss: 0.0132 - val_accuracy: 0.9965 - val_f1: 0.9519\n57/57 [==============================] - 70s 1s/step - loss: 0.0069 - accuracy: 0.9981 - f1: 0.9549 - val_loss: 0.0131 - val_accuracy: 0.9967 - val_f1: 0.9688\n57/57 [==============================] - 69s 1s/step - loss: 0.0066 - accuracy: 0.9982 - f1: 0.9558 - val_loss: 0.0136 - val_accuracy: 0.9963 - val_f1: 0.9430\n57/57 [==============================] - 69s 1s/step - loss: 0.0064 - accuracy: 0.9982 - f1: 0.9569 - val_loss: 0.0128 - val_accuracy: 0.9966 - val_f1: 0.9512\n57/57 [==============================] - 69s 1s/step - loss: 0.0062 - accuracy: 0.9983 - f1: 0.9580 - val_loss: 0.0132 - val_accuracy: 0.9965 - val_f1: 0.9636\n57/57 [==============================] - 69s 1s/step - loss: 0.0061 - accuracy: 0.9983 - f1: 0.9607 - val_loss: 0.0128 - val_accuracy: 0.9967 - val_f1: 0.9604\n57/57 [==============================] - 70s 1s/step - loss: 0.0058 - accuracy: 0.9984 - f1: 0.9615 - val_loss: 0.0130 - val_accuracy: 0.9966 - val_f1: 0.9560\n57/57 [==============================] - 70s 1s/step - loss: 0.0056 - accuracy: 0.9984 - f1: 0.9611 - val_loss: 0.0133 - val_accuracy: 0.9965 - val_f1: 0.9611\n57/57 [==============================] - 69s 1s/step - loss: 0.0054 - accuracy: 0.9985 - f1: 0.9621 - val_loss: 0.0128 - val_accuracy: 0.9967 - val_f1: 0.9680\n57/57 [==============================] - 69s 1s/step - loss: 0.0051 - accuracy: 0.9986 - f1: 0.9648 - val_loss: 0.0132 - val_accuracy: 0.9965 - val_f1: 0.9633\n57/57 [==============================] - 69s 1s/step - loss: 0.0050 - accuracy: 0.9986 - f1: 0.9623 - val_loss: 0.0130 - val_accuracy: 0.9966 - val_f1: 0.9674\n57/57 [==============================] - 69s 1s/step - loss: 0.0048 - accuracy: 0.9987 - f1: 0.9632 - val_loss: 0.0129 - val_accuracy: 0.9967 - val_f1: 0.9718\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate\n\nThe code below evaluates your model on the development data using accuracy (which is not very indicative on this task. To get better insights, store the model output and run your own evaluation.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test data using `evaluate`\n# Careful: Really high even if the model only predicts the majority class\n\nresults = model_bilstm_lstm.evaluate(dev_tokens, np.array(dev_tags), batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T13:11:40.477440Z","iopub.execute_input":"2023-12-13T13:11:40.478228Z","iopub.status.idle":"2023-12-13T13:18:02.484528Z","shell.execute_reply.started":"2023-12-13T13:11:40.478187Z","shell.execute_reply":"2023-12-13T13:18:02.483329Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"3453/3453 [==============================] - 334s 97ms/step - loss: 0.0151 - accuracy: 0.9963 - f1: 0.9729\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Get model predictions","metadata":{}},{"cell_type":"code","source":"# Get predictions on development set\ny_pred = model_bilstm_lstm.predict(dev_tokens)\n\n# get dimension index with highest prob (--> label)\ny_pred = np.argmax(y_pred, axis=-1)\ny_dev =  np.argmax(dev_tags, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T13:18:02.485905Z","iopub.execute_input":"2023-12-13T13:18:02.486243Z","iopub.status.idle":"2023-12-13T13:18:12.808854Z","shell.execute_reply.started":"2023-12-13T13:18:02.486214Z","shell.execute_reply":"2023-12-13T13:18:12.807743Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"108/108 [==============================] - 10s 88ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions per token:\n# map labels back to tokens\n\ndef output_to_file(dev_tokens, y_pred, output_path):\n    \n    with open(output_path, 'w') as outfile:\n        for token,  preds in zip(dev_tokens, y_pred):\n            for tok, pred in zip(token, preds):\n                # igonre padding:\n                if tok in idx2token:\n                    tok_str = idx2token[tok]\n                    outfile.write(f'{tok_str}\\t{idx2tag[pred]}\\n')\n    \noutput_to_file(dev_tokens, y_pred, \"lstmout.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T13:18:12.810491Z","iopub.execute_input":"2023-12-13T13:18:12.810931Z","iopub.status.idle":"2023-12-13T13:18:13.023146Z","shell.execute_reply.started":"2023-12-13T13:18:12.810891Z","shell.execute_reply":"2023-12-13T13:18:13.022098Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open(\"lstm.pkl\", 'wb') as f:\n        pickle.dump(model_bilstm_lstm, f)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T13:18:13.024445Z","iopub.execute_input":"2023-12-13T13:18:13.024744Z","iopub.status.idle":"2023-12-13T13:18:13.385211Z","shell.execute_reply.started":"2023-12-13T13:18:13.024719Z","shell.execute_reply":"2023-12-13T13:18:13.384346Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}